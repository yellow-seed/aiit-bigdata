{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分散表現を特徴量とした分類\n",
    "\n",
    "- word2vecのベクトルの平均値をテキストのベクトルとして分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# matplotlib: 日本語フォントの設定\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.family'] = 'sans-serif'\n",
    "rcParams['font.sans-serif'] = ['Hiragino Maru Gothic Pro', 'Yu Gothic', 'Meirio', \n",
    "                               'Takao', 'IPAexGothic', 'IPAPGothic', 'Noto Sans CJK JP']\n",
    "\n",
    "# Spacyの日本語モデルをロード\n",
    "nlp = spacy.load('ja_core_news_lg')\n",
    "\n",
    "# フィードデータの読み込み、確認\n",
    "feeds = pd.read_csv('data/output_jp.csv')\n",
    "\n",
    "# title から　'\\u300012/4 10:35更新'　の部分を削除\n",
    "feeds['title'] = feeds['title'].map(lambda x: x[:x.rfind('\\u3000')] if x[-2:] == '更新' else x)\n",
    "\n",
    "# text と summary を結合\n",
    "feeds['text'] = feeds['title'].str.cat(feeds['summary'], sep='。', na_rep='')\n",
    "\n",
    "# 不要になった列を削除した処理用の DataFrame\n",
    "df = feeds.drop(['title', 'summary'], axis=1)\n",
    "\n",
    "# 確認\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 教師データの作成\n",
    "\n",
    "URLに基づいた教師ラベルの設定：\n",
    "\n",
    "label 0\n",
    "- business\n",
    "- economy\n",
    "- politics\n",
    "- cat4 (nhk)\n",
    "- cat5 (nhk)\n",
    "\n",
    "label 1\n",
    "- culture\n",
    "- science\n",
    "- sport\n",
    "- cat2 (nhk)\n",
    "- cat7 (nhk)\n",
    "\n",
    "label 2\n",
    "- 上記以外"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_0 = ['business', 'economy', 'politics', 'cat4', 'cat5']\n",
    "label_1 = ['culture', 'science', 'sport', 'cat2', 'cat7']\n",
    "\n",
    "# label_0 の単語が url の中に含まれている：0\n",
    "# label_1 の単語が url の中に含まれている：1\n",
    "# それ以外：2\n",
    "def get_label(url):\n",
    "    url = url.lower()\n",
    "    # map: label_0 の単語それぞれについて、url の中に含まれているか真偽を返す\n",
    "    # any: mapの結果について論理和をとる\n",
    "    if any(map(lambda x: x in url, label_0)):\n",
    "        return 0\n",
    "    if any(map(lambda x: x in url, label_1)):\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# df['url'] について get_label を適用した結果を df['label'] として追加\n",
    "df['label'] = df['url'].map(lambda x: get_label(x))\n",
    "# 各ラベルの数を確認\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label 0, 1 を取り出す\n",
    "df = df.query('label != 2')\n",
    "# 数を確認\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 日本語テキストに対する前処理\n",
    "\n",
    "- 表記の正規化\n",
    "- トークン化（形態素解析）\n",
    "- ストップワードの除去\n",
    "- 見出し語化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不要な単語を除去\n",
    "# - ストップワード (is_stop)\n",
    "# - いくつかの品詞\n",
    "#     AUX: 助動詞\n",
    "#     PUNCT: 句読点\n",
    "#     SPACE: 空白文字\n",
    "#     SYM: 記号\n",
    "#     X: その他\n",
    "# - うまく取り除けない単語や文字\n",
    "stop_pos = ['AUX', 'PUNCT', 'SPACE', 'SYM', 'X']\n",
    "stop_words = ['.']\n",
    "\n",
    "def token_to_add(w):\n",
    "    t = w.text    # 単語\n",
    "    p = w.pos_    # 品詞\n",
    "    l = w.lemma_  # 原型\n",
    "\n",
    "    # ストップワードは None を返す\n",
    "    if w.is_stop:\n",
    "        return None\n",
    "    if p in stop_pos:\n",
    "        return None\n",
    "    if l in stop_words:\n",
    "        return None\n",
    "\n",
    "    if len(l) == 0:\n",
    "        return t\n",
    "    return l\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = []\n",
    "    \n",
    "    for w in nlp(text):\n",
    "        t = token_to_add(w)\n",
    "        if t is not None:\n",
    "            tokens.append(t)\n",
    "\n",
    "    # トークンのリストを返す\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキストのベクトル化\n",
    "\n",
    "- word2vecのベクトルの平均値をテキストのベクトルとする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストのトークンのword2vecベクトルの平均値を計算\n",
    "def text2vec(text):\n",
    "    tokens = preprocess(text)\n",
    "    total = nlp.vocab[' '].vector\n",
    "    n = len(tokens)\n",
    "    for t in tokens:\n",
    "        vec = nlp.vocab[t].vector\n",
    "        total = total + vec\n",
    "    # ndarrayのベクトルをlistにして返す\n",
    "    return (total / n).tolist()\n",
    "\n",
    "# mapの結果はSeriesになるため、ndarrayからlistに変換することでlistのlistができ、\n",
    "# それをDataFrameにする\n",
    "vector = pd.DataFrame(df['text'].map(lambda x: text2vec(x)).values.tolist())\n",
    "# 確認\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分類モデルの作成（学習）\n",
    "\n",
    "- ナイーブベイズ分類器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# 説明変数、目的変数\n",
    "X = vector\n",
    "Y = df.label\n",
    "\n",
    "# ナイーブベイズ分類器\n",
    "model = GaussianNB()\n",
    "\n",
    "# 学習\n",
    "model.fit(X, Y)\n",
    "# 教師データについてモデルからの予測値を計算\n",
    "Y_predict = model.predict(X)\n",
    "\n",
    "# 特異度の計算\n",
    "matrix = confusion_matrix(Y, Y_predict)\n",
    "specificity = matrix[0, 0] / (matrix[0, 1] + matrix[0, 0])\n",
    "# 精度\n",
    "print('正確度: {:.3f}, 適合度: {:.3f}, 再現率: {:.3f}, 特異度: {:.3f}'.format(\n",
    "    accuracy_score(Y, Y_predict), precision_score(Y, Y_predict),\n",
    "    recall_score(Y, Y_predict), specificity))\n",
    "# ROC, AUC\n",
    "Y_proba = model.predict_proba(X)\n",
    "fpr, tpr, thresholds = roc_curve(Y, Y_proba[:, 1])\n",
    "plt.plot(fpr, tpr, label='AUC={:.3f}'.format(auc(fpr, tpr)))\n",
    "plt.xlabel('偽陽性率 (FP率)')\n",
    "plt.ylabel('真陽性率 (TP率)')\n",
    "plt.title('ROC曲線')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交差検証法により予測精度を検証\n",
    "\n",
    "- ナイーブベイズ分類器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 交差検証の実行\n",
    "score = cross_val_score(model, X, Y, cv=10, scoring='roc_auc')\n",
    "print('AUC={:.3f} (+/- {:.3f})'.format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### その他の分類器\n",
    "\n",
    "- ロジスティック回帰\n",
    "- 決定木\n",
    "- ランダムフォレスト\n",
    "- SVM、カーネルSVM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
