{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 次元圧縮\n",
    "\n",
    "次元圧縮\n",
    "- 文章をトピックの集まりとすることで特徴量を削減\n",
    "\n",
    "潜在的意味解析 (LSA) \n",
    "- 同じように使われる単語をトピックとしてまとめる方法\n",
    "\n",
    "<hr>\n",
    "\n",
    "![LSA](slides/LSA.png)\n",
    "\n",
    "- 行列 U：各文書におけるトピックの擬似的な頻度\n",
    "- 行列 S：重要度（対角成分）\n",
    "- 行列 V：各トピックに単語が現れる度合い\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# matplotlib: 日本語フォントの設定\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.family'] = 'sans-serif'\n",
    "rcParams['font.sans-serif'] = ['Hiragino Maru Gothic Pro', 'Yu Gothic', 'Meirio', \n",
    "                               'Takao', 'IPAexGothic', 'IPAPGothic', 'Noto Sans CJK JP']\n",
    "\n",
    "# 日本語モデル\n",
    "nlp = spacy.load('ja_core_news_lg')\n",
    "\n",
    "# フィードデータの読み込み、確認\n",
    "feeds = pd.read_csv('data/output_jp.csv')\n",
    "\n",
    "# title と summary を結合\n",
    "# str.cat() により複数列の文字列を結合\n",
    "# - sep=' ': 間に挟む文字列\n",
    "# - na_rep='': NaN は空文字列に変換（指定しないと結合結果が NaN になる）\n",
    "feeds['text'] = feeds['title'].str.cat(feeds['summary'], sep='。', na_rep='')\n",
    "\n",
    "# 不要になった列を削除した処理用の DataFrame\n",
    "df = feeds.drop(['title', 'summary'], axis=1)\n",
    "\n",
    "# 確認\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 日本語テキストに対する前処理\n",
    "\n",
    "- 表記の正規化\n",
    "- トークン化（形態素解析）\n",
    "- ストップワードの除去\n",
    "- 見出し語化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不要な単語を除去\n",
    "# - ストップワード (is_stop)\n",
    "# - いくつかの品詞\n",
    "#     AUX: 助動詞\n",
    "#     PUNCT: 句読点\n",
    "#     SPACE: 空白文字\n",
    "#     SYM: 記号\n",
    "#     X: その他\n",
    "# - うまく取り除けない単語や文字\n",
    "stop_pos = ['AUX', 'PUNCT', 'SPACE', 'SYM', 'X']\n",
    "stop_words = ['.']\n",
    "\n",
    "def token_to_add(w):\n",
    "    t = w.text    # 単語\n",
    "    p = w.pos_    # 品詞\n",
    "    l = w.lemma_  # 原型\n",
    "\n",
    "    # ストップワードは None を返す\n",
    "    if w.is_stop:\n",
    "        return None\n",
    "    if p in stop_pos:\n",
    "        return None\n",
    "    if l in stop_words:\n",
    "        return None\n",
    "\n",
    "    if len(l) == 0:\n",
    "        return t\n",
    "    return l\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = []\n",
    "    \n",
    "    for w in nlp(text):\n",
    "        t = token_to_add(w)\n",
    "        if t is not None:\n",
    "            tokens.append(t)\n",
    "\n",
    "    # トークンのリストを返す\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキストのベクトル化\n",
    "\n",
    "- Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期化\n",
    "vectorizer = CountVectorizer(tokenizer=preprocess)\n",
    "# ベクトル化\n",
    "vector = vectorizer.fit_transform(df.text)\n",
    "# テキストをベクトル化した結果の DataFrame\n",
    "df_vector = pd.DataFrame.sparse.from_spmatrix(vector)\n",
    "# 確認\n",
    "df_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語の DataFrame\n",
    "df_words = pd.DataFrame(vectorizer.get_feature_names_out())\n",
    "# 列インデックス\n",
    "df_words.columns = ['word']\n",
    "# 確認\n",
    "df_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テキスト数に対し単語数の方が（圧倒的に）多いと、アルゴリズムによってはうまく動かなくなる場合が出てくる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA: 潜在的意味解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# 初期化\n",
    "# - n_components=100: トピック数（100はドキュメントでの推奨値）\n",
    "model = TruncatedSVD(n_components=100)\n",
    "\n",
    "# トピックへの分解\n",
    "# - スライドの行列 U\n",
    "lsa_features = model.fit_transform(vector)\n",
    "\n",
    "# トピックによる説明割合\n",
    "# - 割合が大きくなるようにトピック数を選択することも行われる\n",
    "print('explained_variance_ratio: {:.3f}'.format(model.explained_variance_ratio_.sum()))\n",
    "\n",
    "# サイズを確認\n",
    "print('shape:', lsa_features.shape)\n",
    "\n",
    "# pandas DataFrame 化\n",
    "df_lsa = pd.DataFrame(lsa_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語とトピックを対応させたDataFrameを作成\n",
    "# - model.components_: 各トピックにおける単語との対応（スライドの行列 V）\n",
    "df_components = pd.concat([df_words, pd.DataFrame(model.components_).T], axis=1)\n",
    "df_components = df_components.set_index('word')\n",
    "df_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最初の10個をトピックの重要度と合わせて表示\n",
    "# - 重要度: スライドの行列 S の対角成分\n",
    "# - 単語: 各トピックに単語が現れる度合い\n",
    "# df_components.sort_values(i, ascending=False)\n",
    "# - i 列を降順でソート\n",
    "for i in range(0, 10):\n",
    "    print('[{}] {:.2f} {}'.format(i, model.singular_values_[i],\n",
    "                                  df_components.sort_values(i, ascending=False).head(10).index.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 非階層的クラスタリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# クラスタ数\n",
    "N_clusters = 10\n",
    "\n",
    "# KMeansによりクラスタリング\n",
    "# - lsa_features: スライドの行列 U 各文書におけるトピックの擬似的な頻度\n",
    "# - normalize() により単位ベクトル化\n",
    "clusters = KMeans(n_clusters=N_clusters).fit_predict(normalize(lsa_features))\n",
    "\n",
    "# 結果を DataFrame にまとめる\n",
    "df_cluster = pd.DataFrame(clusters, columns=['cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クラスタごとの棒グラフ（頻度）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 頻度の棒グラフ\n",
    "for i in range(0, N_clusters):\n",
    "    print('Cluster', i)\n",
    "    _cdf = df_cluster[df_cluster['cluster'] == i]\n",
    "    df_counts = pd.concat([df_words, df_vector.iloc[_cdf.index].sum()], axis=1)\n",
    "    df_counts.columns=['word', 'counts']\n",
    "    df_bar = df_counts.sort_values('counts', ascending=False).head(10)\n",
    "    sns.barplot(x=df_bar.counts, y=df_bar.word, orient='h')\n",
    "    # 各クラスタごとの主要 3 トピックから上位 10 単語を表示\n",
    "    for j in df_lsa.iloc[_cdf.index].mean().sort_values(ascending=False).head(3).index:\n",
    "        print('[{}] {}'.format(j, df_components.sort_values(j, ascending=False).head(10).index.values))\n",
    "    # 棒グラフの表示\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_0 = ['business', 'economy', 'politics', 'cat4', 'cat5']\n",
    "label_1 = ['culture', 'science', 'sport', 'cat2', 'cat7']\n",
    "\n",
    "# label_0 の単語が url の中に含まれている：0\n",
    "# label_1 の単語が url の中に含まれている：1\n",
    "# それ以外：2\n",
    "def get_label(url):\n",
    "    url = url.lower()\n",
    "    # map: label_0 の単語それぞれについて、url の中に含まれているか真偽を返す\n",
    "    # any: mapの結果について論理和をとる\n",
    "    if any(map(lambda x: x in url, label_0)):\n",
    "        return 0\n",
    "    if any(map(lambda x: x in url, label_1)):\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# df['url'] について get_label を適用した結果を df['label'] として追加\n",
    "df['label'] = df['url'].map(lambda x: get_label(x))\n",
    "\n",
    "# label 0, 1 を取り出す\n",
    "df = df.query('label != 2')\n",
    "\n",
    "# 初期化\n",
    "vectorizer = CountVectorizer(tokenizer=preprocess)\n",
    "\n",
    "# ベクトル化\n",
    "vector = vectorizer.fit_transform(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 説明変数、目的変数\n",
    "# - vector が sparse のため toarray() により dense に変換\n",
    "X = vector.toarray()\n",
    "Y = df.label\n",
    "\n",
    "# ナイーブベイズ分類器\n",
    "model = MultinomialNB()\n",
    "\n",
    "# 交差検証の実行\n",
    "score = cross_val_score(model, X, Y, scoring='roc_auc')\n",
    "print('AUC={:.3f} (+/- {:.3f})'.format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トピックの分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 初期化\n",
    "# - n_components=100: トピック数（100はドキュメントでの推奨値）\n",
    "LSA = TruncatedSVD(n_components=100)\n",
    "\n",
    "# トピックへの分解\n",
    "lsa_features = LSA.fit_transform(vector)\n",
    "\n",
    "# lsa_features には負の値も出てくるため [0, 1] に正規化\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(lsa_features)\n",
    "\n",
    "# ナイーブベイズ分類器\n",
    "model = MultinomialNB()\n",
    "\n",
    "# 交差検証の実行\n",
    "score = cross_val_score(model, X, Y, scoring='roc_auc')\n",
    "print('AUC={:.3f} (+/- {:.3f})'.format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 18:24:45) \n[Clang 12.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "d451e6fd3e3a90eb92742bb146e8e20c8a00180dad068226f141e79828259f8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
