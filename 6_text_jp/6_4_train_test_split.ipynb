{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 日本語テキストデータの分類\n",
    "\n",
    "日本語ニュースフィードのカテゴリを教師データとし、カテゴリを予測\n",
    "\n",
    "1. テキストの段階で教師データとテストデータに分割\n",
    "2. 教師データからモデルを作成\n",
    "3. テストデータについて精度を検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# matplotlib: 日本語フォントの設定\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.family'] = 'sans-serif'\n",
    "rcParams['font.sans-serif'] = ['Hiragino Maru Gothic Pro', 'Yu Gothic', 'Meirio', \n",
    "                               'Takao', 'IPAexGothic', 'IPAPGothic', 'Noto Sans CJK JP']\n",
    "\n",
    "# 日本語モデル\n",
    "nlp = spacy.load('ja_core_news_lg')\n",
    "\n",
    "# フィードデータの読み込み、確認\n",
    "feeds = pd.read_csv('data/output_jp.csv')\n",
    "\n",
    "# title と summary を結合\n",
    "# str.cat() により複数列の文字列を結合\n",
    "# - sep=' ': 間に挟む文字列\n",
    "# - na_rep='': NaN は空文字列に変換（指定しないと結合結果が NaN になる）\n",
    "feeds['text'] = feeds['title'].str.cat(feeds['summary'], sep='。', na_rep='')\n",
    "\n",
    "# 不要になった列を削除した処理用の DataFrame\n",
    "df = feeds.drop(['title', 'summary'], axis=1)\n",
    "\n",
    "# 確認\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 教師データの作成\n",
    "\n",
    "URLに基づいた教師ラベルの設定：\n",
    "\n",
    "label 0\n",
    "- business\n",
    "- economy\n",
    "- politics\n",
    "- cat4 (nhk)\n",
    "- cat5 (nhk)\n",
    "\n",
    "label 1\n",
    "- culture\n",
    "- science\n",
    "- sport\n",
    "- cat2 (nhk)\n",
    "- cat7 (nhk)\n",
    "\n",
    "label 2\n",
    "- 上記以外"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_0 = ['business', 'economy', 'politics', 'cat4', 'cat5']\n",
    "label_1 = ['culture', 'science', 'sport', 'cat2', 'cat7']\n",
    "\n",
    "# label_0 の単語が url の中に含まれている：0\n",
    "# label_1 の単語が url の中に含まれている：1\n",
    "# それ以外：2\n",
    "def get_label(url):\n",
    "    url = url.lower()\n",
    "    # map: label_0 の単語それぞれについて、url の中に含まれているか真偽を返す\n",
    "    # any: mapの結果について論理和をとる\n",
    "    if any(map(lambda x: x in url, label_0)):\n",
    "        return 0\n",
    "    if any(map(lambda x: x in url, label_1)):\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# df['url'] について get_label を適用した結果を df['label'] として追加\n",
    "df['label'] = df['url'].map(lambda x: get_label(x))\n",
    "# 各ラベルの数を確認\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label 0, 1 を取り出す\n",
    "df = df.query('label != 2')\n",
    "# 数を確認\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 日本語テキストに対する前処理\n",
    "\n",
    "- 表記の正規化\n",
    "- トークン化（形態素解析）\n",
    "- ストップワードの除去\n",
    "- 見出し語化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不要な単語を除去\n",
    "# - ストップワード (is_stop)\n",
    "# - いくつかの品詞\n",
    "#     AUX: 助動詞\n",
    "#     PUNCT: 句読点\n",
    "#     SPACE: 空白文字\n",
    "#     SYM: 記号\n",
    "#     X: その他\n",
    "# - うまく取り除けない単語や文字\n",
    "stop_pos = ['AUX', 'PUNCT', 'SPACE', 'SYM', 'X']\n",
    "stop_words = ['.']\n",
    "\n",
    "def token_to_add(w):\n",
    "    t = w.text    # 単語\n",
    "    p = w.pos_    # 品詞\n",
    "    l = w.lemma_  # 原型\n",
    "\n",
    "    # ストップワードは None を返す\n",
    "    if w.is_stop:\n",
    "        return None\n",
    "    if p in stop_pos:\n",
    "        return None\n",
    "    if l in stop_words:\n",
    "        return None\n",
    "\n",
    "    if len(l) == 0:\n",
    "        return t\n",
    "    return l\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = []\n",
    "    \n",
    "    for w in nlp(text):\n",
    "        t = token_to_add(w)\n",
    "        if t is not None:\n",
    "            tokens.append(t)\n",
    "\n",
    "    # トークンのリストを返す\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テキストの段階で教師データとテストデータに分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_train, text_test, Y_train, Y_test = train_test_split(df.text, df.label, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキストのベクトル化\n",
    "\n",
    "- Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期化\n",
    "vectorizer = CountVectorizer(tokenizer=preprocess)\n",
    "\n",
    "# 教師データのベクトル化\n",
    "vector = vectorizer.fit_transform(text_train)\n",
    "\n",
    "# テストデータのベクトル化\n",
    "vector_test = vectorizer.transform(text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分類モデルの作成（学習）\n",
    "\n",
    "- ナイーブベイズ分類器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# 説明変数、目的変数\n",
    "# - vector が sparse のため toarray() により dense に変換\n",
    "X = vector.toarray()\n",
    "\n",
    "# ナイーブベイズ分類器\n",
    "# - 特徴量（説明変数）は整数のカウントデータ（単語の出現頻度など）\n",
    "model = MultinomialNB()\n",
    "\n",
    "# 学習\n",
    "model.fit(X, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テストデータについて精度を検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# テストデータの説明変数\n",
    "X_test = vector_test.toarray()\n",
    "\n",
    "# テストデータについてモデルからの予測値を計算\n",
    "Y_predict = model.predict(X_test)\n",
    "\n",
    "# 精度\n",
    "print('正確度: {:.3f}, 適合度: {:.3f}, 再現率: {:.3f}'.format(\n",
    "    accuracy_score(Y_test, Y_predict), precision_score(Y_test, Y_predict), recall_score(Y_test, Y_predict)))\n",
    "# ROC, AUC\n",
    "Y_proba = model.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, Y_proba[:, 1])\n",
    "plt.plot(fpr, tpr, label='AUC={:.3f}'.format(auc(fpr, tpr)))\n",
    "plt.xlabel('偽陽性率 (FP率)')\n",
    "plt.ylabel('真陽性率 (TP率)')\n",
    "plt.title('ROC曲線')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 18:24:45) \n[Clang 12.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "d451e6fd3e3a90eb92742bb146e8e20c8a00180dad068226f141e79828259f8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
