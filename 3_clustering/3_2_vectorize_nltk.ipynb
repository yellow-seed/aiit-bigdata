{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テキストのベクトル化\n",
    "\n",
    "- Bag of Words (BoW)\n",
    "- TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://feeds.bbci.co.uk/news/rss.xml</td>\n",
       "      <td>Conservative peer Michelle Mone to take leave ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://feeds.bbci.co.uk/news/rss.xml</td>\n",
       "      <td>Ambulance staff to strike on 21 December. Serv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://feeds.bbci.co.uk/news/rss.xml</td>\n",
       "      <td>Strep A schools may be given preventive antibi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://feeds.bbci.co.uk/news/rss.xml</td>\n",
       "      <td>Eddie Jones sacked by England after review int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://feeds.bbci.co.uk/news/rss.xml</td>\n",
       "      <td>Tattooists and beauty salons replace banks on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>http://feeds.bbci.co.uk/news/science_and_envir...</td>\n",
       "      <td>Nasa's Orion capsule makes safe return to Eart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>http://feeds.bbci.co.uk/news/science_and_envir...</td>\n",
       "      <td>One of Central America's most active volcanoes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>http://feeds.bbci.co.uk/news/technology/rss.xml</td>\n",
       "      <td>December 2024 set as date for universal phone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>http://feeds.bbci.co.uk/news/technology/rss.xml</td>\n",
       "      <td>Twitter's paid blue tick re-launches after pau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>http://feeds.bbci.co.uk/news/technology/rss.xml</td>\n",
       "      <td>S Korea says crypto-fugitive Do Kwon is in Ser...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>788 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   url  \\\n",
       "0                 http://feeds.bbci.co.uk/news/rss.xml   \n",
       "1                 http://feeds.bbci.co.uk/news/rss.xml   \n",
       "2                 http://feeds.bbci.co.uk/news/rss.xml   \n",
       "3                 http://feeds.bbci.co.uk/news/rss.xml   \n",
       "4                 http://feeds.bbci.co.uk/news/rss.xml   \n",
       "..                                                 ...   \n",
       "783  http://feeds.bbci.co.uk/news/science_and_envir...   \n",
       "784  http://feeds.bbci.co.uk/news/science_and_envir...   \n",
       "785    http://feeds.bbci.co.uk/news/technology/rss.xml   \n",
       "786    http://feeds.bbci.co.uk/news/technology/rss.xml   \n",
       "787    http://feeds.bbci.co.uk/news/technology/rss.xml   \n",
       "\n",
       "                                                  text  \n",
       "0    Conservative peer Michelle Mone to take leave ...  \n",
       "1    Ambulance staff to strike on 21 December. Serv...  \n",
       "2    Strep A schools may be given preventive antibi...  \n",
       "3    Eddie Jones sacked by England after review int...  \n",
       "4    Tattooists and beauty salons replace banks on ...  \n",
       "..                                                 ...  \n",
       "783  Nasa's Orion capsule makes safe return to Eart...  \n",
       "784  One of Central America's most active volcanoes...  \n",
       "785  December 2024 set as date for universal phone ...  \n",
       "786  Twitter's paid blue tick re-launches after pau...  \n",
       "787  S Korea says crypto-fugitive Do Kwon is in Ser...  \n",
       "\n",
       "[788 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# フィードデータの読み込み\n",
    "feeds = pd.read_csv('data/output_en.csv')\n",
    "\n",
    "# title と summary を結合して text 列を作成\n",
    "feeds['text'] = feeds['title'].str.cat(feeds['summary'], sep='. ', na_rep='')\n",
    "\n",
    "# 不要になった列を削除した処理用の DataFrame\n",
    "df = feeds.drop(['title', 'summary'], axis=1)\n",
    "\n",
    "# 確認\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 英語テキストに対する前処理\n",
    "\n",
    "以下をまとめて行う関数 preprocess() を定義\n",
    "- トークン化（単語に分割）\n",
    "- 小文字化\n",
    "- ストップワードの除去\n",
    "- ステミング\n",
    "- 見出し語化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_to_remove = r'[\"`,.' + r\"'\" + r']'\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words += [\"'\", '\"', ':', ';', '.', ',', '-', '!', '?', \"'s\", '`', '•', '%']\n",
    "stop_words += ['–', '—', '‘', '’', '“', '”', '…', '|', '#', '$', '&', \"''\", '(', ')']\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "# 品詞の名称を変換\n",
    "def wordnet_tag(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    return None\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = []\n",
    "    # 品詞のタグ付けをした各トークンについて\n",
    "    for t in nltk.pos_tag(nltk.tokenize.word_tokenize(text.replace('-', ' '))):\n",
    "        # 小文字化\n",
    "        t0 = t[0].lower()\n",
    "        # 不要な文字の削除\n",
    "        t0 = re.sub(symbols_to_remove, '', t0)\n",
    "        # 空文字列になったら次へ\n",
    "        if t0 == '':\n",
    "            continue\n",
    "        # stop_words に含まれていないトークンのみを残す\n",
    "        if t0 in stop_words:\n",
    "            continue\n",
    "        # カンマ区切りが入った数値からカンマを削除\n",
    "        if t[1] == 'CD':\n",
    "            t0 = t0.replace(',', '')\n",
    "        # 見出し語化\n",
    "        tag = wordnet_tag(t[1])\n",
    "        if tag is None:\n",
    "            t0 = lemmatizer.lemmatize(t0)\n",
    "        else:\n",
    "            t0 = lemmatizer.lemmatize(t0, tag)\n",
    "        # ステミング\n",
    "        t0 = stemmer.stem(t0)\n",
    "        # リストに追加\n",
    "        tokens.append(t0)\n",
    "    # トークンのリストを返す\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキストのベクトル化 (1)\n",
    "\n",
    "- Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 例として最初の2行のテキストを処理\n",
    "text_list = [df['text'].iloc[0], df['text'].iloc[1]]\n",
    "\n",
    "# CountVectorizer\n",
    "# - tokenizer=preprocess: トークン化処理に上で定義した preprocess を使用することを指定\n",
    "vectorizer = CountVectorizer(tokenizer=preprocess)\n",
    "\n",
    "# ベクトル化\n",
    "vector = vectorizer.fit_transform(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conservative peer Michelle Mone to take leave of absence from Lords. Baroness Mone is accused of benefitting from a company she recommended for a Covid contract.\n",
      "  (0, 10)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 19)\t2\n",
      "  (0, 26)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 17)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 21)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 11)\t1\n"
     ]
    }
   ],
   "source": [
    "# 1行目\n",
    "print(text_list[0])\n",
    "print(vector[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conserv peer michel mone take leav absenc lord baro accus benefit compani recommend covid contract "
     ]
    }
   ],
   "source": [
    "# ベクトルの単語との対応\n",
    "for i in vector[0].indices:\n",
    "    # print()\n",
    "    # - end=' ': 改行の代わりに空白を出力\n",
    "    print(vectorizer.get_feature_names_out()[i], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['conserv', 1],\n",
       " ['peer', 1],\n",
       " ['michel', 1],\n",
       " ['mone', 2],\n",
       " ['take', 1],\n",
       " ['leav', 1],\n",
       " ['absenc', 1],\n",
       " ['lord', 1],\n",
       " ['baro', 1],\n",
       " ['accus', 1],\n",
       " ['benefit', 1],\n",
       " ['compani', 1],\n",
       " ['recommend', 1],\n",
       " ['covid', 1],\n",
       " ['contract', 1]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 単語と頻度\n",
    "[[vectorizer.get_feature_names_out()[i], vector[0, i]] for i in vector[0].indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambulance staff to strike on 21 December. Services across England and Wales affected, but life-threatening calls will be responded to.\n",
      "  (0, 5)\t1\n",
      "  (0, 24)\t1\n",
      "  (0, 25)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 28)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 27)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 22)\t1\n"
     ]
    }
   ],
   "source": [
    "# 2行目\n",
    "print(text_list[1])\n",
    "print(vector[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambul staff strike 21 decemb servic across england wale affect life threaten call respond "
     ]
    }
   ],
   "source": [
    "# ベクトルの単語との対応\n",
    "for i in vector[1].indices:\n",
    "    print(vectorizer.get_feature_names_out()[i], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['conserv', 1],\n",
       " ['peer', 1],\n",
       " ['michel', 1],\n",
       " ['mone', 2],\n",
       " ['take', 1],\n",
       " ['leav', 1],\n",
       " ['absenc', 1],\n",
       " ['lord', 1],\n",
       " ['baro', 1],\n",
       " ['accus', 1],\n",
       " ['benefit', 1],\n",
       " ['compani', 1],\n",
       " ['recommend', 1],\n",
       " ['covid', 1],\n",
       " ['contract', 1]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 単語と頻度\n",
    "[[vectorizer.get_feature_names_out()[i], vector[0, i]] for i in vector[0].indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキストのベクトル化 (2)\n",
    "\n",
    "- TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 例として最初の2行のテキストを処理\n",
    "text_list = [df['text'].iloc[0], df['text'].iloc[1]]\n",
    "\n",
    "# TfidfVectorizer\n",
    "# - tokenizer=preprocess: トークン化処理に上で定義した preprocess を使用することを指定\n",
    "vectorizer = TfidfVectorizer(tokenizer=preprocess)\n",
    "\n",
    "# ベクトル化\n",
    "vector = vectorizer.fit_transform(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1行目\n",
    "print(text_list[0])\n",
    "print(vector[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベクトルの単語との対応\n",
    "for i in vector[0].indices:\n",
    "    print(vectorizer.get_feature_names_out()[i], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語とTF-IDF\n",
    "[[vectorizer.get_feature_names_out()[i], vector[0, i]] for i in vector[0].indices]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 18:24:45) \n[Clang 12.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "d451e6fd3e3a90eb92742bb146e8e20c8a00180dad068226f141e79828259f8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
